#!/usr/bin/env ruby
#
################################################################################
# A script that parses user's notes and backups web pages, blog articles and
# tools.
################################################################################

require 'uri'
require 'securerandom'

CHROME_BIN = ENV['CHROME_BIN']

URL_SEARCH_PATTERN = %r{https?://(www\.)?[-a-zA-Z0-9@:%._+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b([-a-zA-Z0-9()@:%_+.~#?&//=]*)}
                     .freeze

def normalize_hostname(hostname)
  hostname.start_with?('www.') ? hostname[4..] : hostname
end

def output_file(uri)
  uri.fragment = nil
  output_file_stem = File.basename(uri.path, File.extname(uri.path))
  output_file_stem.chomp!('/')
  output_file_stem = SecureRandom.uuid if output_file_stem.empty?
  "#{output_file_stem}.pdf"
end

def download_file(url, output_path)
  puts "Downloading: #{url}"
  system(CHROME_BIN, '--headless', "--print-to-pdf=#{output_path}",
         url)
end

def main
  search_root_dir = File.realpath(ARGV[0] || Dir.pwd)
  puts "Search directory: #{search_root_dir}"

  found_url_results = `rg -i --with-filename -o '#{URL_SEARCH_PATTERN}' \
        --type=md --type=txt '#{search_root_dir}'`

  tasks = []
  found_url_results.each_line do |line|
    line.chomp!
    file, url = line.split(':', 2)

    file_resolved = File.realpath(file)
    uri = URI.parse(url)

    case normalize_hostname(uri.host)
    when 'github.com'
      # TODO
    else
      output_dir = File.dirname(file_resolved)
      output_path = File.join(output_dir, output_file(uri))
      tasks << Thread.new do
        download_file(uri, output_path)
      end
    end
  ensure
    sleep(1)
  end
  tasks.each(&:join)
end

main if __FILE__ == $PROGRAM_NAME
